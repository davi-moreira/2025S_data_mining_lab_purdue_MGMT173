[
  {
    "objectID": "index.html#course-description-and-objectives",
    "href": "index.html#course-description-and-objectives",
    "title": "MGMT 17300: Data Mining Lab",
    "section": "Course Description and Objectives",
    "text": "Course Description and Objectives\nThis course emphasizes exposure to diverse data mining methods, with a particular focus on experimental designs and predictive modeling techniques. It provides a comprehensive understanding of the roles that data and analytics disciplines play in business decision-making. Through the analysis of synthetic, business-focused datasets, students will engage in a hands-on experiential learning process that highlights the practical applications of these methods. This approach underscores the interconnected nature of experimental and predictive techniques within the broader analytics landscape, enabling students to develop a nuanced appreciation of data-driven decision-making. By the end of the course, students will be better equipped to thoughtfully select advanced analytics courses and strategically navigate their paths toward specialized or advanced training in fields aligned with data analytics.\nCourse Website: https://davi-moreira.github.io/2025S_data_mining_purdue_MGMT173/\n\nInstructor: Professor Davi Moreira\n\nEmail: dmoreira@purdue.edu\nVirtual Office hours - Zoom link in your Course Brightspace Page\nIndividual Appointments: Book time with me through the link in the course syllabus on your Course Brightspace Page or by appointment.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "MGMT 17300: Data Mining Lab",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the conclusion of this course, students will be able to:\n\nUnderstand the Role of Data Analytics Disciplines:\nDevelop a comprehensive understanding of the interconnected roles of data management, experimental design, and predictive modeling in supporting contemporary data-driven decision-making.\nRecognize the Importance of Structured Data and Infrastructure:\nAcknowledge the critical role of structured business data and enterprise data management/analytics infrastructure in enabling efficient and effective data-driven business strategies.\nApply Data Mining Methods:\nGain foundational knowledge and hands-on experience in using diverse data mining techniques, with a focus on experimental designs and predictive modeling, to solve business problems.\nUse Analytics Tools Proficiently:\nUtilize powerful analytics tools (such as R and other programming languages) to analyze synthetic business-focused datasets and apply data mining methods in practical contexts.\nNavigate Advanced Analytics Pathways:\nMake informed decisions about advanced coursework and specialization in analytics by leveraging a nuanced appreciation of the experimental and predictive methods introduced in the course.\nBridge Theory and Practice:\nApply theoretical insights to practical scenarios through experiential learning with synthetic datasets, fostering skills that support informed decision-making in real-world business environments.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "MGMT 17300: Data Mining Lab",
    "section": "Course Materials",
    "text": "Course Materials\nTextbooks (for reference):\n1. Modern Data-Driven Decision Making: with practices in data mining and R, by Zhiwei Zhu, © Copyright Digital and AI Literacies 2023. (Draft version available in the course Brightspace page)\n2. R for Data Science 2e, by Hadley Wickham, Mine Cetinkaya-Rundel, and Garrett Grolemund\n- Online version here\n3. James, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An Introduction to Statistical Learning with Applications in R/Python. Springer. Download here\nComputing (Required):\n- A laptop or desktop with internet access and the capability to install and run R and RStudio.\nSoftware (Required):\n- The R language and RStudio will be used in this course. For details, see R and RStudio.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-infra-structure",
    "href": "index.html#course-infra-structure",
    "title": "MGMT 17300: Data Mining Lab",
    "section": "Course Infra-structure",
    "text": "Course Infra-structure\nBrightspace: The Course Brightspace Page https://purdue.brightspace.com/ should be checked on a regular basis for announcements and course material.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "",
    "text": "Basic Operations\nData types\nDataframes\nLogical operators\n\n\n\nR Packages and Their Importance\nData Import in R\nExporting Data from R\nWhere to find Help?"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#overview",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#overview",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nBasic Operations\nData types\nDataframes\nLogical operators\n\n\n\nR Packages and Their Importance\nData Import in R\nExporting Data from R\nWhere to find Help?"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#what-does-coding-really-mean",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#what-does-coding-really-mean",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "What Does Coding Really Mean?",
    "text": "What Does Coding Really Mean?\n\n\n\n\n\nYoutube Video!"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#using-r-as-a-calculator",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#using-r-as-a-calculator",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Using R as a Calculator!",
    "text": "Using R as a Calculator!\n\n# What is the result?\n2 + 9 * 4\n4 + 3 / 10^2\npi - 3\n\n# Scientific notation\n5 * 10^2\n5 * 10^-2\n111111 * 111111\n1111111 * 1111111"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#objects-variables-and-vectors",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#objects-variables-and-vectors",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Objects: Variables and Vectors",
    "text": "Objects: Variables and Vectors\n\n# What is the value of a?\na &lt;- 4\na &lt;- a * 5\na &lt;- a + 10\na &lt;- seq(a, 100, 5)"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#vectors",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#vectors",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Vectors",
    "text": "Vectors\n\nnumbers &lt;- c(1, 4, 10, pi, 1/3)\ntext &lt;- c(\"a\", \"b\", \"Daniels Business School\", \"Purdue University\", \"BAIM\")\n(all_together &lt;- c(numbers, text)) # use parentheses to display the object content\n\nNote that when combining numbers and text, all elements turn into text. We will discuss text handling more later."
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#style",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#style",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Style",
    "text": "Style\n\n\n“Programs must be written for people to read, and only incidentally for machines to execute.”\n— Hal Abelson\n\n\nThe tidy tools manifesto"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#style-1",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#style-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Style",
    "text": "Style\n\nIt is recommended that object names be descriptive. Additionally, adopting a programming style for your data analysis is valuable. It facilitates human reading and interpretation of the code. Let’s look at object names from r4ds. Which one is better?\n\n\ni_use_snake_case\notherPeopleUseCamelCase\nsome.people.use.periods\nAnd_aFew.People_RENOUNCEconvention"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#the-tidyverse-style-guide",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#the-tidyverse-style-guide",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "The tidyverse style guide",
    "text": "The tidyverse style guide\n\nThe tidyverse style guide: this guide not only presents good practices and programming style but is accompanied by two packages that help data scientists maintain code consistency."
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#functions-1",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#functions-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Functions",
    "text": "Functions\nFunctions are the workhorse of statistical programming in R. Many of the analyses we will perform are based on using the correct functions and identifying the appropriate arguments for each case.\nWe have already seen some examples of functions:\n\ninstall.packages() # installs packages\nlibrary() # loads packages into memory\nrequire() # loads packages into memory\nsessionInfo() # information about the R version\n\nThe main use of functions is to automate operations that would take a long time to do manually, be prone to errors, or simply be tedious. For this reason, functions are developed in packages."
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#functions-2",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#functions-2",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Functions",
    "text": "Functions\nFor example, if we need to find the mean between two numbers, we could do the calculation manually:\n\n(35 + 65) / 2\n\nBut if we had 1000 numbers instead of 2, this process would be extremely long and tiring. So, we can simply use the mean() function to calculate the average of all numbers from 1 to 1000:\n\nmean(1:1000)"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#functions-3",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#functions-3",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Functions",
    "text": "Functions\nR has countless functions for doing all kinds of calculations that you can imagine (and even those you can’t). As you progress in using R, there will be specific tasks for which no existing function is satisfactory. In these moments, the advantage of R being a programming language becomes evident — we can create our own functions.\nFor now, let’s explore some of the functions that already exist in R. Did you notice that I didn’t need to type all the numbers from 1 to 1000 in the previous example?\n\nnumbers &lt;- 1:1000\nnumbers_desc &lt;- 1000:1 \n\nMuch easier than numbers &lt;- c(1, 2, 3, ..., 1000)."
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#functions-4",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#functions-4",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Functions",
    "text": "Functions\nBut what if I wanted to find the mean of the odd numbers from 1 to 1000? Would I need to type the numbers one by one?\nIn these moments, remember that laziness is one of the traits that separates good programmers from the rest. Almost all tedious and repetitive tasks in programming can be automated in some way.\nObviously, R has the seq() function that allows us to create a vector of odd numbers. Notice how the arguments of the function are used:\n\nodds &lt;- seq(from = 1, to = 1000, by = 2)\nmean(odds)"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#functions-5",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#functions-5",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Functions",
    "text": "Functions\nR comes pre-installed with several statistical functions — after all, this is one of its main purposes. Besides the mean() function, which we saw earlier, let’s also look at other descriptive statistics:\n\nsd(odds) # standard deviation\nvar(odds) # variance\nrange(odds) # range\nIQR(odds) # interquartile range\n\nThe summary() command gives us an overview of this vector:\n\nsummary(odds)"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#data-types-1",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#data-types-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Data Types",
    "text": "Data Types\n\nProgramming languages store variables under different classes. Today, we will have a general discussion about them so that you know they exist, and we will go into details throughout the course.\n\nNumeric values: double, integer\nText: character\nFactors: factor\nLogical values: logical\nSpecial values: NA, NULL, Inf, NaN\n\nTo discover the type of an object, you can use the typeof() function."
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#dataframes-1",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#dataframes-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Dataframes",
    "text": "Dataframes\nWe can think of dataframes as collections of vectors placed side by side. It is by far the most used format for data analysis and processing.\n\nnames &lt;- c(\"Mary\", \"Davi\", \"Juliana\", \"Gabriel\")\nmajor &lt;- c(\"Engineering\", \"Political Science\", \"Business\", \"Economy\")\ntime_at_company &lt;- c(3, 10, 10, 1)\n\nteam &lt;- data.frame(names, major, time_at_company)\n\nnrow(team) # number of rows\nncol(team) # number of columns\nhead(team) # first observations\nsummary(team) # summary of data\n\nR also supports other data structures like matrices and lists, which we will cover as needed."
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#data-tidying",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#data-tidying",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Data Tidying",
    "text": "Data Tidying\n\n\n\n\n\nObservations in rows\nAttributes in columns\nValues in cells"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#subsetting-vectors",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#subsetting-vectors",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Subsetting Vectors",
    "text": "Subsetting Vectors\nThe tools we will now see are used to “pinpoint” information stored in R’s memory. Returning to the vector of odd numbers we created earlier, suppose I want to know the value of the 287th element:\n\nodds &lt;- seq(from = 1, to = 1000, by = 2)\nodds[287]\n\nWe can expand the [ operator for various selections, as needed:\n\nodds[c(1, 76, 48)] # select various numbers\nodds[-c(1:250)] # all numbers except the first 250\nodds[odds &gt; 900] # only values greater than 900"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#subsetting-dataframes",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#subsetting-dataframes",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Subsetting Dataframes",
    "text": "Subsetting Dataframes\nThe use of the [ operator is similar for dataframes, but we need to pay attention to rows and columns:\n\nteam[1, 3] # row 1, column 3\nteam[1,] # returns all of row 1\nteam[, 3] # returns all of column 3\nteam[, c(1, 3)] # returns columns 1 and 3\n\nFor dataframes, it is very common to use the $ operator to select columns:\n\nteam$time_at_company # selects the variable \"time at company\"\nteam[, 3] # same result"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#logical-operators-1",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#logical-operators-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Logical Operators",
    "text": "Logical Operators\n\nFor more complex selections, it is common to rely on logical operators.\n\n\n\n\n\nLogical"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#logical-operators-2",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#logical-operators-2",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Logical Operators",
    "text": "Logical Operators\n\nThe most common are & and |, but all relational operators are also recognized:\n\n== (equal to)\n!= (not equal to)\n&gt; (greater than)\n&lt; (less than)\n&gt;= (greater than or equal to)\n&lt;= (less than or equal to).\n\n\n\nteam[team$time_at_company == 10,] # only people with 10 years at the company\nteam[team$time_at_company &lt; 5,] # only people with less than 5 years at the company\nteam[team$time_at_company &lt; 5 | team$major == \"Business\",] # less than 5 years at the company OR in and specific area\nteam[team$time_at_company &gt; 2 & team$time_at_company &lt; 5,] # between 2 and 5 years at the company"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#r-packages-and-their-importance",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#r-packages-and-their-importance",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "R Packages and Their Importance",
    "text": "R Packages and Their Importance\n\nR functionality is enhanced by packages—which are like “apps” in the smartphone ecosystem.\n\n# Example of installing a package\ninstall.packages(\"readxl\")\n\n# Loading a package to use its functions\nlibrary(readxl)"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#the-tidyverse-project",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#the-tidyverse-project",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "The Tidyverse Project",
    "text": "The Tidyverse Project\n\n\n\nIn space, no one can hear you scream.\n– Alien (1979)"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#the-tidyverse-project-1",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#the-tidyverse-project-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "The Tidyverse Project",
    "text": "The Tidyverse Project\n\n\n\nThe tidyverse is a collection of R packages designed for data science. All packages share an underlying philosophy and common APIs."
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#the-tidyverse-data-science-workflow",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#the-tidyverse-data-science-workflow",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "The Tidyverse Data Science Workflow",
    "text": "The Tidyverse Data Science Workflow\n\n\n\n\nTidyverse Project"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#installing-packages",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#installing-packages",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Installing Packages",
    "text": "Installing Packages\n\nYou can install packages in two ways:\n\nUsing the R command line with install.packages()\nUsing RStudio, by navigating to the “Packages” tab and searching for the desired package.\n\n\n\n# Installing multiple packages at once\ninstall.packages(c(\"readxl\", \"ggplot2\"))"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#loading-packages",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#loading-packages",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Loading Packages",
    "text": "Loading Packages\n\nAfter installation, you must load a package to use it:\n\n# Loading the readxl package\nlibrary(readxl)\n\n\n\nNote: Some commonly used packages for data analysis are ggplot2, dplyr, and tidyr."
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#data-import-in-r-1",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#data-import-in-r-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Data Import in R",
    "text": "Data Import in R\n\nR allows importing data from various file formats like text files, Excel sheets, CSV files, and SQL databases."
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#importing-text-files",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#importing-text-files",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Importing Text Files",
    "text": "Importing Text Files\n\n\n# Importing a text file\ndata &lt;- read.table(\"data.txt\", header = TRUE, sep = \",\")"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#importing-excel-files",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#importing-excel-files",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Importing Excel Files",
    "text": "Importing Excel Files\n\nMake sure you have the readxl package installed:\n\n# Importing data from an Excel file\nlibrary(readxl)\ndata &lt;- read_excel(\"data/SalesData.xlsx\")\n\n\nTo import a specific sheet:\n\n# Importing a specific sheet from an Excel file\ndata &lt;- read_excel(\"c:/mydata/data.xlsx\", sheet = \"Sheet1\")"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#importing-csv-files",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#importing-csv-files",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Importing CSV Files",
    "text": "Importing CSV Files\n\nCSV files are commonly used and can be easily imported using read.csv():\n\n# Importing data from a CSV file\ndata &lt;- read.csv(\"data.csv\")"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#importing-data-from-sql-databases",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#importing-data-from-sql-databases",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Importing Data from SQL Databases",
    "text": "Importing Data from SQL Databases\n\nTo import data from a SQL database, you can use the RODBC package:\n\n# Importing from SQL database\nlibrary(RODBC)\nconn &lt;- odbcConnect(\"database_name\")\ndata &lt;- sqlQuery(conn, \"SELECT * FROM table_name\")\nodbcClose(conn)\n\n\n\nNote: For this course, we will primarily focus on importing data from Excel spreadsheets."
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#exporting-data-from-r-1",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#exporting-data-from-r-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Exporting Data from R",
    "text": "Exporting Data from R\n\nOnce your data analysis is complete, you’ll often need to export the data for further use or reporting. R provides several ways to export datasets to various formats, including CSV, Excel, and text files."
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#exporting-data-to-csv-files",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#exporting-data-to-csv-files",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Exporting Data to CSV Files",
    "text": "Exporting Data to CSV Files\n\nOne of the most common way to export data from R is to save it as a CSV file using the write.csv() function.\n\n# Exporting a dataset to a CSV file\nwrite.csv(data, \"output_data.csv\", row.names = FALSE)\n\n\nThe first argument is the data you want to export.\nThe second argument is the file name (path) for the exported file.\nrow.names = FALSE avoids adding an extra column for row numbers."
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#exporting-data-to-excel-files",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#exporting-data-to-excel-files",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Exporting Data to Excel Files",
    "text": "Exporting Data to Excel Files\n\nYou can export data to Excel using the writexl package. First, make sure it’s installed.\n\n# Install the writexl package\ninstall.packages(\"writexl\")\n\n# Exporting a dataset to an Excel file\nlibrary(writexl)\nwrite_xlsx(data, \"data/output_data.xlsx\")\n\n\n\nNote: The write_xlsx() function saves the data into an Excel file, and you can specify the file path."
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#exporting-data-to-text-files",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#exporting-data-to-text-files",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Exporting Data to Text Files",
    "text": "Exporting Data to Text Files\n\nFor exporting data to a text file, you can use the write.table() function. This is particularly useful when you want to use a delimiter other than commas, such as tabs.\n\n# Exporting a dataset to a tab-delimited text file\nwrite.table(data, \"output_data.txt\", sep = \"\\t\", row.names = FALSE)\n\n\nThe sep argument specifies the delimiter used in the file (in this case, tabs)."
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#exporting-data-to-rds-files",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#exporting-data-to-rds-files",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Exporting Data to RDS Files",
    "text": "Exporting Data to RDS Files\n\nRDS is a format specific to R that allows you to save R objects and reload them later.\n\n# Exporting data to an RDS file\nsaveRDS(data, \"data.rds\")\n\n# Loading the RDS file back into R\ndata &lt;- readRDS(\"data.rds\")\n\n\n\nNote: RDS files are useful when you want to save R objects for later use within R itself."
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#where-to-find-help-1",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#where-to-find-help-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Where to find Help?",
    "text": "Where to find Help?\nThe sum() function is often useful. It allows you to sum vectors. Let’s take the opportunity to consult the documentation of this function through another function, ?.\n\n?sum()"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#where-to-find-help-2",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#where-to-find-help-2",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Where to find Help?",
    "text": "Where to find Help?"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#where-to-find-help-3",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#where-to-find-help-3",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Where to find Help?",
    "text": "Where to find Help?\nBesides R’s official documentation, a very valuable resource is Stack Overflow.\n\n\n\n\nStack"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#where-to-find-help-4",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#where-to-find-help-4",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Where to find Help?",
    "text": "Where to find Help?\n\n\n\n\n\n\n\n\n\n\n\n\nClaude AI\nChatGPT\nCopilot AI\nGitHub Copilot\nCursor AI"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#where-to-find-help-5",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#where-to-find-help-5",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Where to find Help?",
    "text": "Where to find Help?\n\n\nswirl teaches you R programming and data science interactively"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#where-to-find-help-6",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#where-to-find-help-6",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Where to find Help?",
    "text": "Where to find Help?\n\n\nRStudio Primers"
  },
  {
    "objectID": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#summary-1",
    "href": "lecture_slides/03_data_structure_operations/03_data_structure_operations.html#summary-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Summary",
    "text": "Summary\n\n\n\nFunctions in R\n\nAutomating tasks with functions\nPredefined functions (e.g., mean(), sd(), summary())\nCreating custom functions for specific tasks\n\nData Structures\n\nData types in R: numeric, text, factors, logical values\nDataframes: the most used data structure for analysis\n\nPackages are essential in extending R’s functionality.\nYou can install and load packages easily with install.packages() and library() functions.\nR supports importing data from multiple sources, including text files, Excel sheets, CSV files, and SQL databases.\nYou can export datasets to various formats in R, including CSV, Excel, text files, and RDS.\n\nwrite.csv() and write_xlsx() are common functions for CSV and Excel exports.\nwrite.table() allows for more customizable exports, such as tab-delimited files.\nUse saveRDS() and readRDS() for saving and reloading R-specific objects."
  },
  {
    "objectID": "syllabus.html#course-description-and-objectives",
    "href": "syllabus.html#course-description-and-objectives",
    "title": "Syllabus",
    "section": "Course Description and Objectives",
    "text": "Course Description and Objectives\nThis course emphasizes exposure to diverse data mining methods, with a particular focus on experimental designs and predictive modeling techniques. It provides a comprehensive understanding of the various roles that data and analytics disciplines play in business decision-making. Through the analysis of numerous synthetic, business-focused datasets, students will engage in a hands-on experiential learning process that highlights the practical applications of these methods. This approach underscores the interconnected nature of experimental and predictive techniques within the broader analytics landscape, enabling students to develop a nuanced appreciation of data-driven decision-making. By the end of the course, students will be better equipped to thoughtfully select advanced analytics courses and strategically navigate their paths toward specialized or advanced training in fields aligned with data analytics.\nCourse Website: https://davi-moreira.github.io/2025S_data_mining_lab_purdue_MGMT173/",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#instructor",
    "href": "syllabus.html#instructor",
    "title": "Syllabus",
    "section": "Instructor",
    "text": "Instructor\n\nInstructor: Professor Davi Moreira\n\nOffice: Young Hall 414\nEmail: dmoreira@purdue.edu\nVirtual Office hours: Zoom link in your Course Brightspace Page\nIndividual Appointments: Book time with me through the link in the course syllabus on your Course Brightspace Page or by appointment.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#learning-outcomes",
    "href": "syllabus.html#learning-outcomes",
    "title": "Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the conclusion of this course, students will be able to:\n\nUnderstand the Role of Data Analytics Disciplines:\nDevelop a comprehensive understanding of the interconnected roles of data management, experimental design, and predictive modeling in supporting contemporary data-driven decision-making.\nRecognize the Importance of Structured Data and Infrastructure:\nAcknowledge the critical role of structured business data and enterprise data management/analytics infrastructure in enabling efficient and effective data-driven business strategies.\nApply Data Mining Methods:\nGain foundational knowledge and hands-on experience in using diverse data mining techniques, with a focus on experimental designs and predictive modeling, to solve business problems.\nUse Analytics Tools Proficiently:\nUtilize powerful analytics tools (such as R and other programming languages) to analyze synthetic business-focused datasets and apply data mining methods in practical contexts.\nNavigate Advanced Analytics Pathways:\nMake informed decisions about advanced coursework and specialization in analytics by leveraging a nuanced appreciation of the experimental and predictive methods introduced in the course.\nBridge Theory and Practice:\nApply theoretical insights to practical scenarios through experiential learning with synthetic datasets, fostering skills that support informed decision-making in real-world business environments.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course Materials",
    "text": "Course Materials\nTextbooks (for reference):\n1. Modern Data-Driven Decision Making: with practices in data mining and R, by Zhiwei Zhu, © Copyright Digital and AI Literacies 2023. (Draft version available in the course Brightspace page)\n2. R for Data Science 2e, by Hadley Wickham, Mine Cetinkaya-Rundel, and Garrett Grolemund\n- Online version here\n3. James, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An Introduction to Statistical Learning with Applications in R/Python. Springer. Download here\nComputing (Required):\n- A laptop or desktop with internet access and the capability to install and run R and RStudio.\nSoftware (Required):\n- The R language and RStudio will be used in this course. For details, see R and RStudio.\n\nCourse Infra-structure\nBrightspace: The Course Brightspace Page https://purdue.brightspace.com/ should be checked on a regular basis for announcements and course material.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assessments",
    "href": "syllabus.html#assessments",
    "title": "Syllabus",
    "section": "Assessments",
    "text": "Assessments\nAs part of a university-wide initiative, the Business School has adopted an Official Grading Policy that caps the overall class GPA at 3.3. Final letter grades are determined by curving final percentages, subject to any extra-credit exceptions discussed in this syllabus. While you will see your final percentage in Brightspace, individual grade thresholds will not be disclosed before official submissions.\n\n\n\nAssessment\nWeight\n\n\n\n\nAttendance/Participation\n10%\n\n\nQuizzes\n25%\n\n\nHomework\n25%\n\n\nOnline Midterm Exams\n20%\n\n\nFinal Project\n20%\n\n\n\n\nAttendance and Participation\nAttend class, participate in activities, and complete any participatory exercises. Random attendance checks will be used to measure involvement. According to Purdue regulations, students are expected to attend every class/lab meeting for which they are registered.\n\n\nQuizzes\nRegular quizzes based on lecture material will be administered, with no drops. Due dates and details will be on Brightspace. Quizzes help reinforce content and maintain steady engagement.\n\n\nHomework\nHomework assignments offer practical, hands-on exposure to data mining tasks. Expect multiple-choice questions requiring analysis of provided results. Deadlines will be posted in Brightspace. These assignments are crucial for building technical and analytical skills.\n\n\nOnline Midterm Exams\nTwo midterm exams, administered online, incorporate the methods used in quizzes and homework but in a more comprehensive format. They are open-book and open-notes. Students have 60 minutes to complete multiple-choice questions that test both technical proficiency and conceptual understanding. The second midterm is not cumulative. Makeup exams are granted only for verifiable, exceptional circumstances (e.g., serious personal medical emergency, family death, NCAA conflict).\n\n\nFinal Project\nIn groups, students will complete a practical data mining project, culminating in a team presentation submitted via Brightspace. The project provides an opportunity to apply course concepts to a real-world scenario, demonstrating analytical and problem-solving skills. Detailed guidelines will be provided according to the course schedule.\n\n\nGrade Challenges\nGrades and solutions will be posted soon after each assignment deadline. Students have 7 calendar days from the grade posting to submit any challenge (3 days for the final two quizzes and homework assignments). Challenges must be based on legitimate discrepancies regarding data mining principles or grading accuracy.\n\nReview posted solutions thoroughly.\n\nIf you suspect an error, email Dr. Moreira with:\n\nCourse name, section, and lecture day/time\n\nYour name and Student ID\n\nAssignment/Exam Title or Number\n\nSpecific deduction questioned\n\nClear rationale referencing solutions or rubrics\n\n\n\nNo grades will be discussed in-class. Please use office hours for clarifications. After the 7-day (or 3-day) window, grades are final.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies-and-additional-details",
    "href": "syllabus.html#course-policies-and-additional-details",
    "title": "Syllabus",
    "section": "Course Policies and Additional Details",
    "text": "Course Policies and Additional Details\n\nExtra Credit Opportunities\n\nCheck the Course Syllabus document on Brightspace for details.\n\n\n\nAI Policy\n\nYou may use AI tools to support your learning (e.g., clarifying concepts, generating examples), but:\n\nDo not use AI for requesting solutions or exams.\n\nPractice refining prompts to get better AI outputs.\n\nVerify all AI-generated content for accuracy.\n\nCite any AI usage in your documents.\n\n\n\n\nAdditional Information\nRefer to Brightspace for deadlines, academic integrity policies, accommodations, CAPS information, and non-discrimination statements.\n\n\nSubject to Change Policy\nWhile we will endeavor to maintain the course schedule, the syllabus may be adjusted to accommodate the learning pace and needs of the class.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "Syllabus",
    "section": "Schedule",
    "text": "Schedule",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\nTopic\nMaterial\nSupplementary Materials\n\n\n\n\nWeek 1\nSyllabus, Logistics, and Introduction to R and RStudio\nslides\n- Video: The NBA Data Scientist- Video: Hans Rosling’s 200 Countries, 200 Years- Video: RStudio is now Posit!\n\n\nWeek 2\nData Organization to Data Driven Decisions\nslides\n- Video: Relational vs. Non-Relational Databases\n\n\nWeek 3\nBasic Operations and Data Structures\nslidesscript\n- R Data Types- Dates with Lubridate- Categorical Data\n\n\nWeek 4\nData Wrangling\nslidesscript\n.\n\n\nWeek 5\nExploratory Data Analysis (EDA) with Plots and Maps\nslidesscript\n- R4DS: Exploratory Data Analysis - IBM: What is Exploratory Data Analysis?- R4DS: Visualize - Hadley Wickham’s Lecture - r-graph-gallery - ggplot flipbook - Edward Tufte\n\n\nWeek 6\nOnline Midterm 01: No class, No Office-Hours\n.\n.\n\n\nWeek 7\nSupervised Learning: Linear Regression\nslidesscript\n- Video: Fitting a Line with Least Squares Regression - Video: Least Squares Regression - Video: Inference for Linear Regression -IMS: Linear regression with a single predictor -IMS: Inference for linear regression with a single predictor- Video: Introduction to Multiple Regression- IMS: Inference for linear regression with multiple predictors\n\n\nWeek 8\nSupervised Learning: Logistic Regression and Model Evaluation\nslidesscript\n- Video: Model Selection in Multiple Regression- Video: Checking Multiple Regression Diagnostics Using Graphs-IMS: Linear regression with multiple predictors-IMS: Inference for linear regression with multiple predictors- R Package: Variable Selection Methods - IMS: Model Selection\n\n\nWeek 09\nUnsupervised Learning: Clustering\nslidesscript\n.\n\n\nWeek 10\nOnline Midterm 02: No class, No Office-Hours\n.\n.\n\n\nWeek 11\nFinal Project Guidelines Presentation\n.\n.\n\n\nWeek 12\nFinal Project Preparation – in class\n.\n.\n\n\nWeek 13\nCommunicating Results\nslidesscript\n- Video: Quarto Dashboards 1: Hello, Dashboards! - Video: Shiny New Things Using R Bridge the Gap in Electronic Medical Record Reporting\n\n\nWeek 14\nFinal Project Preparation – in class\n.\n.\n\n\nWeek 15\nFinal Project Presentation: No Office-Hours\n.\n.",
    "crumbs": [
      "Schedule and Material"
    ]
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#overview",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#overview",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nAssociation Between Two Variables\nModel\nSupervised and Unsupervised Learning\nSupervised Learning: Regression\nSimple Linear Regression Example: Advertising Spend\n\n\n\nMultiple Regression Model\nMultiple Regression Model Example\nComparing Single and Multiple Regression Models\nPredicting Sales with a Regression Model\nComparing Prediction Performance"
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#covariance",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#covariance",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Covariance",
    "text": "Covariance\n\n\n\n\n\nThe Covariance is a measure of the linear association between two variables.\nPositive values indicate a positive relationship.\nNegative values indicate a negative relationship."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#correlation-coefficient",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#correlation-coefficient",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Correlation Coefficient",
    "text": "Correlation Coefficient\n\n\n\n\nCorrelation is a unit-free measure of linear association and not necessarily causation.\nThe coefficient can take on values between −1 and +1.\n\nValues near −1 indicate a strong negative linear relationship.\nValues near +1 indicate a strong positive linear relationship.\n\nThe closer the correlation is to zero, the weaker the linear relationship."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#what-is-a-model",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#what-is-a-model",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "What is a model?",
    "text": "What is a model?\n\n\n\n\n\n\n\nAll models are wrong, but some are useful.\nGeorge Box\n\n\n\n\n\n\n\n\n\n\n\n\nNY City Subway Map"
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#what-does-it-mean-to-model-data",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#what-does-it-mean-to-model-data",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "What does it mean to “model” data?",
    "text": "What does it mean to “model” data?\n\n\nLet’s start with a very simple premise:\n\nto model, we need to make explicit the conditions under which a variable \\(X\\) is related to a variable \\(Y\\).\n\n\n\n\nLet’s begin by giving specific names to these variables:\n\nDependent Variable (DV): This is our phenomenon of interest, usually denoted as \\(Y\\).\nIndependent Variable (IV): This is the phenomenon that explains/describe our dependent variable, generally denoted as \\(X\\)."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#what-does-it-mean-to-model-data-1",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#what-does-it-mean-to-model-data-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "What does it mean to “model” data?",
    "text": "What does it mean to “model” data?\n\nMathematically, we model \\(Y\\) as a function of \\(X\\). Statistically, modeling can serve two main purposes:\n\nPrediction: The possibility of using the values of \\(X\\) to predict the value of \\(Y\\). There must be a substantive connection between these two variables for one to generate reliable predictions about the values of the other.\nExplanation: Used to understand the connection and significance (both substantive and statistical) of the relationship between two variables. In this case, we aim to accurately estimate the impact of one variable on the other, preferably excluding any potential omitted variables."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#creating-a-simulated-tech-dataset-1",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#creating-a-simulated-tech-dataset-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Creating a Simulated Tech Dataset",
    "text": "Creating a Simulated Tech Dataset\n\nWe simulate a dataset that represents a hypothetical tech company’s business data. This dataset includes key variables that influence sales, based on realistic business scenarios. The aim is to illustrate how these variables interact and impact overall sales performance.\nVariables:\n\nAdvertising Spend (adv_spend): Monthly advertising budget, reflecting marketing investment (in thousands of USD).\nProduct Complexity (prod_complex): A measure of the complexity of the product, scored on a scale from 1 (least complex) to 10 (most complex).\nCustomer Support Rating (support_rating): A quality rating for customer support services, scored from 1 (poor) to 5 (excellent).\nCustomer Satisfaction Index (cust_satisfaction): A measure of overall customer satisfaction, scaled from 0 to 100.\nNumber of Active Users (active_users): The total number of active users (in thousands).\nProduct Price (product_price): The price of the product (in USD).\nWebsite Traffic (website_traffic): Number of website visits (in thousands).\nCustomer Acquisition Cost (CAC) (cac): The cost to acquire a new customer (in USD).\nChurn Rate (churn_rate): The percentage of customers lost over a given period.\nPurchase Frequency (purchase_freq): The average number of purchases per customer per month.\nSales (sales): The dependent variable, representing monthly revenue (in thousands of USD)."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#creating-a-simulated-tech-dataset-2",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#creating-a-simulated-tech-dataset-2",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Creating a Simulated Tech Dataset",
    "text": "Creating a Simulated Tech Dataset\n\n# 1. Set the seed for reproducibility to ensure consistent results\nset.seed(123)\n\n# 2. Define the sample size, representing monthly observations for a year or more\nn &lt;- 500\n\n# 3. Generate synthetic predictors\n# Simulate advertising spend (normally distributed around a mean of 50k USD with 10k variability)\nadv_spend &lt;- rnorm(n, mean = 50, sd = 10)  \n\n# Simulate product complexity scores (uniformly distributed between 1 and 10)\nprod_complex &lt;- runif(n, min = 1, max = 10)\n\n# Simulate customer support ratings (uniformly distributed between 1 and 5)\nsupport_rating &lt;- runif(n, min = 1, max = 5)\n\n# Simulate customer satisfaction index (scaled from 0 to 100)\ncust_satisfaction &lt;- rnorm(n, mean = 75, sd = 10)\n\n# Simulate number of active users (thousands)\nactive_users &lt;- rpois(n, lambda = 300)\n\n# Simulate product price (USD)\nproduct_price &lt;- rnorm(n, mean = 500, sd = 50)\n\n# Simulate website traffic (thousands of visits)\nwebsite_traffic &lt;- rpois(n, lambda = 20)\n\n# Simulate customer acquisition cost (CAC in USD)\ncac &lt;- rnorm(n, mean = 150, sd = 20)\n\n# Simulate churn rate (as a percentage)\nchurn_rate &lt;- runif(n, min = 0, max = 20)\n\n# Simulate average purchase frequency (number of purchases per customer per month)\npurchase_freq &lt;- runif(n, min = 1, max = 10)\n\n# 4. Create a sales response variable based on a linear combination of predictors\n# Adding noise to simulate variability in sales\nsales &lt;- 200 + 0.9 * adv_spend - 0.7 * prod_complex + 1.5 * support_rating +\n          0.3 * cust_satisfaction + 0.05 * active_users + 0.1 * website_traffic -\n          0.5 * churn_rate + 0.8 * purchase_freq - 0.2 * cac + rnorm(n, mean = 0, sd = 5)  # Random noise\n\n# 5. Combine all variables into a single data frame\nmy_tech_data &lt;- data.frame(\n  adv_spend = adv_spend,      # Advertising spend\n  prod_complex = prod_complex, # Product complexity\n  support_rating = support_rating, # Customer support rating\n  cust_satisfaction = cust_satisfaction, # Customer satisfaction index\n  active_users = active_users, # Number of active users\n  product_price = product_price, # Product price\n  website_traffic = website_traffic, # Website traffic\n  cac = cac, # Customer acquisition cost\n  churn_rate = churn_rate, # Churn rate\n  purchase_freq = purchase_freq, # Purchase frequency\n  sales = sales               # Monthly sales\n)\n\n# Preview the first few rows of the dataset\n# head(my_tech_data)"
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#summary-statistics",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#summary-statistics",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n# Summary statistics\nsummary(my_tech_data)"
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#sales-vs.-advertising-spend",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#sales-vs.-advertising-spend",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Sales vs. Advertising Spend",
    "text": "Sales vs. Advertising Spend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsiderations:\n\nPositive correlation between advertising spend and sales.\nThe linear trend suggests that higher advertising spend increases sales."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#sales-vs.-product-complexity",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#sales-vs.-product-complexity",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Sales vs. Product Complexity",
    "text": "Sales vs. Product Complexity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsiderations:\n\nProduct complexity shows a slightly negative impact on sales.\nHigher complexity may deter some customers, balancing enterprise and retail preferences."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#sales-vs.-customer-support-rating",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#sales-vs.-customer-support-rating",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Sales vs. Customer Support Rating",
    "text": "Sales vs. Customer Support Rating\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsiderations:\n\nClear positive correlation between support rating and sales.\nImproved support ratings can significantly boost customer retention and revenue."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#correlation-matrix",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#correlation-matrix",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Correlation Matrix",
    "text": "Correlation Matrix\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsiderations:\n\nThe correlation matrix visually represents the relationships between variables.\nHigher correlations (closer to 1 or -1) indicate stronger linear relationships, while values near 0 suggest weak or no linear association."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#importance",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#importance",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Importance",
    "text": "Importance\n\nSupervised Learning\n\nOften used for tasks where the goal is to predict a known outcome (e.g., predicting sales figures, estimating demand for a particular product).\n\nIn business contexts, supervised techniques are employed to forecast revenue, detect fraud, or personalize marketing campaigns.\n\nUnsupervised Learning\n\nHelpful when you have unlabeled data and want to discern inherent structures (e.g., customer segmentation, anomaly detection).\n\nBusinesses might use clustering methods to group customers by similar attributes (purchase behavior) or uncover patterns in massive, unlabeled data."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#difference-between-supervised-and-unsupervised-learning",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#difference-between-supervised-and-unsupervised-learning",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Difference between Supervised and Unsupervised Learning",
    "text": "Difference between Supervised and Unsupervised Learning\n\nSupervised Learning\n\nHas labeled data (i.e., a known outcome/target).\n\nThe algorithm “learns” the relationship between input features (predictors) and the output labels.\n\nUnsupervised Learning\n\nHas no labeled outcome.\n\nThe algorithm attempts to discover hidden patterns or structures within the data."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#supervised-learning-regression-1",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#supervised-learning-regression-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Supervised Learning: Regression",
    "text": "Supervised Learning: Regression\n\nDefinition\n\nRegression is a statistical approach to model the relationship between a numerical outcome (dependent variable) and one or more explanatory (independent) variables.\n\n\nKey Applications\n\nForecasting: Sales, stock prices, housing values.\n\nMarketing Campaign Impact: Evaluating the effect of promotions on revenue.\n\nProduct Features: Examining how features drive user retention.\n\n\n\nTypes of Linear Regression\n\nSimple (Single) Linear Regression\n\nOne independent variable predicting a single continuous dependent variable.\n\nMultiple Linear Regression\n\nTwo or more independent variables predicting a single continuous dependent variable."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#interpreting-the-model-output",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#interpreting-the-model-output",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Interpreting the Model Output",
    "text": "Interpreting the Model Output\n\n\nResiduals:\n\nMeasure the difference between the actual and predicted values.\nSummary statistics (Min, 1Q, Median, 3Q, Max) provide insights into the distribution of residuals.\n\nCoefficients:\n\nIntercept: The predicted sales value when advertising spend is zero.\nadv_spend: For every additional $1,000 spent on advertising, sales are predicted to increase by approximately $0.93K.\n\nSignificance:\n\nPr(&gt;|t|) values test whether coefficients are significantly different from zero.\n*** indicates strong evidence that adv_spend is a significant predictor at the 0.001 level of significance.\n\nResidual Standard Error (RSE):\n\nMeasures the average amount by which the predictions differ from the actual values.\nLower RSE values indicate better model fit.\n\nR-squared and Adjusted R-squared:\n\nR-squared: 73.66% of the variability in sales is explained by advertising spend.\nAdjusted R-squared accounts for the number of predictors and penalizes overfitting.\n\nF-statistic:\n\nTests the overall significance of the model.\nA large F-statistic and small p-value (&lt;2e-16) indicate the model fits the data well."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#assumptions-of-linear-regression",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#assumptions-of-linear-regression",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Assumptions of Linear Regression",
    "text": "Assumptions of Linear Regression\nLinear regression models rely on several key assumptions. Violating these assumptions can impact the reliability and interpretation of the model.\n\nLinearity:\n\nThe relationship between predictors and the dependent variable should be linear.\n\nIndependence of Errors:\n\nObservations should be independent, and residuals should not exhibit autocorrelation.\n\nHomoscedasticity:\n\nThe variance of residuals should remain constant across fitted values.\n\nNormality of Errors:\n\nResiduals should follow a normal distribution.\n\nNo Perfect Multicollinearity:\n\nPredictors should not exhibit perfect or near-perfect linear relationships among themselves."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#importance-in-business-decisions",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#importance-in-business-decisions",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Importance in Business Decisions",
    "text": "Importance in Business Decisions\nLinear regression assumptions are critical to ensure that:\n\nInterpretability:\n\nAssumptions allow coefficients to be interpreted meaningfully, supporting decision-making based on reliable insights.\n\nPredictive Power:\n\nAssumption adherence ensures robust model predictions, reducing errors in business forecasts.\n\nRisk Mitigation:\n\nViolations like multicollinearity can inflate coefficient variances, leading to incorrect conclusions and increased risks.\n\nActionable Insights:\n\nReliable models provide clarity on which factors influence key business metrics (e.g., sales, costs)."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#residuals-vs.-fitted-plot-analysis",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#residuals-vs.-fitted-plot-analysis",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Residuals vs. Fitted Plot Analysis",
    "text": "Residuals vs. Fitted Plot Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinearity:\n\nThe blue smooth line represents the trend of residuals.\nIdeally, the residuals should be randomly scattered around the horizontal line (zero) without a pattern, indicating a linear relationship.\n\nHomoscedasticity:\n\nThe spread of residuals should remain consistent across all fitted values.\nInconsistent spread (funnel shapes) indicates heteroscedasticity, violating the assumption.\n\nOutliers:\n\nPoints significantly away from the zero line or the majority of residuals are potential outliers.\nLabels like 126, 12, and 324 identify influential observations that might need further investigation.\n\n\nConclusion:\n\nThis plot shows a slightly curved trend, suggesting a possible deviation from linearity.\nThe residuals seem relatively homoscedastic, but a formal test may be needed for confirmation.\nInvestigate the labeled outliers to assess their impact on the model."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#normal-q-q-plot-analysis",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#normal-q-q-plot-analysis",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Normal Q-Q Plot Analysis",
    "text": "Normal Q-Q Plot Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormality of Residuals:\n\nThe points should closely follow the diagonal dashed line if the residuals are normally distributed.\nDeviations from the line indicate potential non-normality, especially at the tails.\n\nOutliers:\n\nPoints far from the diagonal line, such as those labeled 126, 324, and 12, suggest possible outliers that might need investigation.\n\n\nConclusion:\n\nThe residuals follow the diagonal line reasonably well, supporting the assumption of normality.\nOutliers at the upper tail should be further examined to assess their influence on the model."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#scale-location-plot-analysis",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#scale-location-plot-analysis",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Scale-Location Plot Analysis",
    "text": "Scale-Location Plot Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomoscedasticity:\n\nThe points should display a random scatter along the blue smooth line.\nA consistent spread indicates homoscedasticity, while a funnel shape or systematic pattern suggests heteroscedasticity.\n\nOutliers:\n\nPoints far from the bulk of the data, such as those labeled 126, 12, and 324, may indicate influential observations.\n\n\nConclusion: - The spread of the residuals appears relatively consistent, supporting the assumption of homoscedasticity. - Outliers should be investigated further to assess their impact on the model."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#cooks-distance-plot-analysis",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#cooks-distance-plot-analysis",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Cook’s Distance Plot Analysis",
    "text": "Cook’s Distance Plot Analysis\n\n\n\n\n\nThis plot helps to identify influential data points that might disproportionately affect the regression model:\n\n\n\n\n\n\n\n\n\n\n\n\nCook’s Distance:\n\nMeasures the impact of removing an observation on the fitted regression model.\nObservations with higher Cook’s Distance values indicate higher influence.\nPoints labeled 126, 113, and 324 appear to have a relatively higher influence.\n\nThreshold for Concern:\n\nA common rule of thumb is that points with Cook’s Distance &gt; \\(\\frac{4}{n}\\) (where \\(n\\) is the number of observations) should be closely examined.\n\n\nConclusion: - Points with higher Cook’s Distance should be investigated for potential data issues or undue influence. - Evaluate whether these points represent valid data or whether they require further investigation or adjustments."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#limitations-of-simple-linear-regression",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#limitations-of-simple-linear-regression",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Limitations of Simple Linear Regression",
    "text": "Limitations of Simple Linear Regression\n\nWhile simple linear regression provides a foundation for modeling relationships, it has several limitations that motivate the use of multiple regression:\n\nSingle Predictor:\n\nSimple regression models only include one predictor variable.\nReal-world phenomena are often influenced by multiple factors.\n\nOmitted Variable Bias:\n\nExcluding important predictors can lead to biased coefficient estimates for the included variable.\n\nInteractions and Nonlinearity:\n\nCannot account for interactions or complex, nonlinear relationships between variables.\n\nLimited Scope for Explanation:\n\nFails to provide a comprehensive view of the factors driving the dependent variable.\n\nRisk of Overemphasis:\n\nMay overstate the importance of the included predictor due to omitted influences."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#why-use-multiple-linear-regression",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#why-use-multiple-linear-regression",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Why Use Multiple Linear Regression?",
    "text": "Why Use Multiple Linear Regression?\n\nIncorporating Multiple Predictors:\n\nIncludes several variables, offering a more realistic and detailed explanation of the dependent variable.\n\nAccounting for Interactions:\n\nModels interactions between variables to capture more complex relationships.\n\nImproved Prediction:\n\nUses additional predictors to enhance accuracy and reduce unexplained variance.\n\nMinimizing Omitted Variable Bias:\n\nReduces bias by including relevant predictors that influence the dependent variable.\n\nPractical Business Insights:\n\nAllows businesses to understand the combined effects of multiple factors (e.g., pricing, marketing, and competition) on outcomes like sales or profits."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#precision-and-accuracy",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#precision-and-accuracy",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Precision and Accuracy",
    "text": "Precision and Accuracy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrecision: Refers to the consistency or reliability of the model’s predictions.\nAccuracy: Refers to how close the model’s predictions are to the true values.\n\n\nIn the context of regression:\n\nHigh Precision, Low Accuracy: Predictions are consistent but biased.\nHigh Precision, High Accuracy: Predictions are both consistent and valid.\nLow Precision, Low Accuracy: Predictions are neither consistent nor valid.\nLow Precision, High Accuracy: Predictions are valid on average but have high variability.\n\n\nTo achieve high precision and high accuracy, we need to meet the model assumptions."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#motivation-controlling-for-a-variable",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#motivation-controlling-for-a-variable",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Motivation: Controlling for a Variable",
    "text": "Motivation: Controlling for a Variable\n\nPuzzle: What is the effect of education on income?\nY: Income\nX: Education\nObjective: X \\(\\rightarrow\\) Y\nChallenge: X \\(\\leftarrow\\) W \\(\\rightarrow\\) Y\nW: IQ (Intelligence)\nSolution: Control for W"
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#motivation-controlling-for-a-variable-1",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#motivation-controlling-for-a-variable-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Motivation: Controlling for a Variable",
    "text": "Motivation: Controlling for a Variable\n\n\n\n\nDAG\n\n\n\nSource: Causal Inference Animated Plots"
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#motivation-controlling-for-a-variable-2",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#motivation-controlling-for-a-variable-2",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Motivation: Controlling for a Variable",
    "text": "Motivation: Controlling for a Variable\n\n\n\n\nRelationship between Y and X controlled for W\n\n\n\nSource: Causal Inference Animated Plots"
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#omitted-variables-confounders",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#omitted-variables-confounders",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Omitted Variables (Confounders)",
    "text": "Omitted Variables (Confounders)\n\nOne of the most common errors in observational studies (besides selection bias and information bias — classification or measurement error);\nIt occurs when we suggest that the explanation for something is “confounded” with the effect of another variable;\nFor example, “the sun rose because the rooster crowed,” and not because of Earth’s rotation."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#how-to-address-omitted-variable-bias",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#how-to-address-omitted-variable-bias",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "How to Address Omitted Variable Bias?",
    "text": "How to Address Omitted Variable Bias?\n\nBe well-versed in the literature;\nSelect good control variables for your model;\nThat is, perform a multiple regression model."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#multiple-regression-model-1",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#multiple-regression-model-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Multiple Regression Model",
    "text": "Multiple Regression Model\nThe equation that describes how the dependent variable \\(y\\) is related to the independent variables \\(x_1, x_2, \\ldots x_p\\) and an error term \\(\\epsilon\\) is:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p + \\epsilon\n\\]\nWhere:\n\n\\(\\beta_0, \\beta_1, \\beta_2, \\dots, \\beta_p\\) are the unknown parameters.\n\\(\\epsilon\\) is a random variable called the error term with the same assumptions as in simple regression (Normality, zero mean, constant variance, independence).\n\\(p\\) is the number of independent variables (dimension or complexity of the model)."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#multiple-regression-equation",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#multiple-regression-equation",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Multiple Regression Equation",
    "text": "Multiple Regression Equation\nThe equation that describes how the mean value of \\(y\\) is related to \\(x_1, x_2, \\ldots x_p\\) is:\n\\[\nE(y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\n\\]\n\\(\\beta_1, \\ldots, \\beta_p\\) measure the marginal effects of the respective independent variables.\n\nFor example, \\(\\beta_1\\) is the change in \\(E(y)\\) corresponding to a 1-unit increase in \\(x_1\\), when all other independent variables are held constant or when we control for all other independent variables."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#estimated-multiple-regression-equation",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#estimated-multiple-regression-equation",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Estimated Multiple Regression Equation",
    "text": "Estimated Multiple Regression Equation\n\n\\[\n\\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \\dots + b_p x_p\n\\]\nA simple random sample is used to compute sample slopes \\(b_0, b_1, b_2, \\dots, b_p\\) that are used as the point estimators of the population slopes \\(\\beta_0, \\beta_1, \\beta_2, \\dots, \\beta_p\\).\n\nHence, \\(\\hat{y}\\) estimates \\(E(Y)\\)."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#multiple-linear-regression-example",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#multiple-linear-regression-example",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Multiple Linear Regression Example",
    "text": "Multiple Linear Regression Example\n\n\n\n\n\nThe multiple linear regression model expands upon the simple linear regression approach by including multiple predictors (independent variables) to explain the variability in the dependent variable. For our hypothetical tech company:\n\nDependent Variable (\\(Y\\)): Sales.\nPredictors (\\(X_1, X_2, X_3\\)):\n\n\\(X_1\\): Advertising Spend.\n\\(X_2\\): Product Complexity.\n\\(X_3\\): Customer Support Rating.\n\nThe model allows us to assess:\n\nThe incremental revenue from additional advertising.\nHow product complexity affects sales.\nThe importance of customer support quality in driving revenue.\n\n\n\n\n\n\n\n# Fit a multiple linear regression model\nfit_multiple &lt;- lm(sales ~ adv_spend + prod_complex + support_rating, \n                   data = my_tech_data)\n\n# Display summary of the model\nsummary(fit_multiple)\n\nKey Takeaways:\n\nAdvertising spend and support rating positively influence sales, while product complexity negatively impacts sales.\nThe model explains a substantial portion of sales variability, supporting its reliability.\nAll predictors are statistically significant and contribute meaningfully to the model."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#multiple-linear-regression-example-1",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#multiple-linear-regression-example-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Multiple Linear Regression Example",
    "text": "Multiple Linear Regression Example\n\n\n\n\n1.Residuals\n\nDefinition: Differences between observed and predicted values of the dependent variable.\nSummary: Provides insights into how well the model fits across the range of observations.\n\n2.Coefficients\n\nIntercept (198.88): Predicted baseline sales when all predictors are set to zero.\nAdvertising Spend (0.93): For every $1,000 increase in advertising spend, sales are predicted to increase by $930, holding other variables constant.\nProduct Complexity (-0.62): A one-unit increase in product complexity is associated with a $620 decrease in sales, assuming other predictors are constant.\nSupport Rating (1.27): A one-point increase in support rating correlates with a $1,270 increase in sales, holding other predictors constant.\n\n3.Significance (Pr(&gt;|t|))\n\nIndicates whether the predictor’s coefficient is significantly different from zero.\nAll predictors are highly significant (p &lt; 0.001).\n\n\n\n\n4.Residual Standard Error (RSE)\n\nMeasures the average deviation of actual sales from predicted sales.\nLower RSE indicates better model fit.\n\n5.R-squared and Adjusted R-squared\n\nR-squared (0.778): 77.8% of the variability in sales is explained by the predictors.\nAdjusted R-squared (0.7767): Accounts for the number of predictors, adjusting for potential overfitting.\n\n6.F-statistic\n\nTests the overall significance of the model.\nA large F-statistic and p-value (&lt; 2.2e-16) indicate the model fits the data well.\n\n\n\n\nXXXXXXXXXXXXXXXXXXXXXXXXXXx"
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#comparing-single-and-multiple-regression-models-1",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#comparing-single-and-multiple-regression-models-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Comparing Single and Multiple Regression Models",
    "text": "Comparing Single and Multiple Regression Models\n\n# Fit a simple linear regression model\nfit_single &lt;- lm(sales ~ adv_spend, data = my_tech_data)\nsummary_single &lt;- summary(fit_single)\n\n# Fit a multiple linear regression model\nfit_multiple &lt;- lm(sales ~ adv_spend + prod_complex + support_rating, data = my_tech_data)\nsummary_multiple &lt;- summary(fit_multiple)"
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#summary-comparison",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#summary-comparison",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Summary Comparison",
    "text": "Summary Comparison\n\nlibrary(Metrics)\n\n# Calculate predictions for single and multiple regression models\npred_single &lt;- predict(fit_single, newdata = my_tech_data)\npred_multiple &lt;- predict(fit_multiple, newdata = my_tech_data)\n\n# Calculate actual sales\nactual_sales &lt;- my_tech_data$sales\n\n# Compute MAE and RMSE for single regression model\nmae_single &lt;- mae(actual_sales, pred_single)\nrmse_single &lt;- rmse(actual_sales, pred_single)\n\n# Compute MAE and RMSE for multiple regression model\nmae_multiple &lt;- mae(actual_sales, pred_multiple)\nrmse_multiple &lt;- rmse(actual_sales, pred_multiple)\n\n# Combine results into a table\nperformance_comparison &lt;- data.frame(\n  Model = c(\"Single Regression\", \"Multiple Regression\"),\n  MAE = c(mae_single, mae_multiple),\n  RMSE = c(rmse_single, rmse_multiple)\n)\n\n# For convenience, pull out R-squared and RSE directly from model summaries:\nr2_single       &lt;- summary_single$r.squared\nr2_multiple     &lt;- summary_multiple$r.squared\nadj_r2_single   &lt;- summary_single$adj.r.squared\nadj_r2_multiple &lt;- summary_multiple$adj.r.squared\nrse_single      &lt;- summary_single$sigma\nrse_multiple    &lt;- summary_multiple$sigma"
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#comparing-single-and-multiple-regression-models-2",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#comparing-single-and-multiple-regression-models-2",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Comparing Single and Multiple Regression Models",
    "text": "Comparing Single and Multiple Regression Models\n\n\n# Fit a simple linear regression model\nfit_single &lt;- lm(sales ~ adv_spend, data = my_tech_data)\nsummary_single &lt;- summary(fit_single)\n\n# Fit a multiple linear regression model\nfit_multiple &lt;- lm(sales ~ adv_spend + prod_complex + support_rating, data = my_tech_data)\nsummary_multiple &lt;- summary(fit_multiple)"
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#summary-comparison-1",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#summary-comparison-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Summary Comparison",
    "text": "Summary Comparison\n\n\n\n\n\n\n\n\n\n\nMetric\nSingle Regression\nMultiple Regression\nConclusion\n\n\n\n\nR-squared\n0.555 (55.5%)\n0.595 (59.5%)\nMultiple regression explains more variability in sales.\n\n\nAdjusted R-squared\n0.554\n0.592\nAdjusted R-squared accounts for model complexity; multiple regression is better.\n\n\nResidual Standard Error\n8.12\n7.76\nLower RSE indicates multiple regression has more precise predictions.\n\n\nMean Absolute Error (MAE)\n6.43\n6.07\nLower MAE suggests multiple regression has less average error.\n\n\nRoot Mean Squared Error (RMSE)\n8.1\n7.73\nLower RMSE confirms multiple regression has less variability in errors."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#key-advantages-of-multiple-regression",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#key-advantages-of-multiple-regression",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Key Advantages of Multiple Regression:",
    "text": "Key Advantages of Multiple Regression:\n\n\n\nCaptures Complex Relationships:\n\nReal-world data often involves multiple factors influencing an outcome simultaneously. A multiple regression model accounts for these relationships to make predictions closer to reality.\n\nImproved Precision:\n\nBy including additional relevant predictors (e.g., product complexity and support rating), the model reduces unexplained variability, leading to more accurate predictions.\n\nAddresses Omitted Variable Bias:\n\nLeaving out important predictors (e.g., support rating) in a single regression can lead to biased estimates. Multiple regression mitigates this bias by incorporating all key factors.\n\nProvides Actionable Insights:\n\nBusinesses can pinpoint specific areas for improvement (e.g., enhancing support rating) that have the highest impact on sales.\n\nScalable Framework:\n\nThe model can easily include new variables as they become relevant, adapting to evolving business environments.\n\n\nPractical Implication:\n\nIn a business context, relying solely on advertising spend to predict sales may ignore other critical factors such as product features or customer satisfaction. Incorporating these predictors helps align the model more closely with real-world decision-making, leading to better strategies and outcomes."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#predicting-sales-with-a-single-regression-model",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#predicting-sales-with-a-single-regression-model",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Predicting Sales with a Single Regression Model",
    "text": "Predicting Sales with a Single Regression Model\nFitting the Model\n\n# Fit a single regression model\nfit_single &lt;- lm(sales ~ adv_spend, data = my_tech_data)\nsummary(fit_single)"
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#prediction-example",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#prediction-example",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Prediction Example",
    "text": "Prediction Example\n\n\n\n\nScenario:\nA business wants to predict sales based on the following value:\n\nAdvertising Spend: 60 (in $1,000)\n\n\n# Define new data for prediction\nnew_data &lt;- data.frame(\n  adv_spend = 60\n)\n\n# Predict sales\npredicted_sales &lt;- predict(fit_single, newdata = new_data)\npredicted_sales\n\n\n\n\n\nInput Variables: The predictor value (advertising spend) is input into the regression equation derived from the fitted model.\nOutput: The predicted sales value is calculated based on the model’s coefficients and the provided input value.\nInterpretation: The result gives an estimate of the expected monthly revenue (in $1,000s) for the specified advertising spend.\n\nKey Takeaway:\n\nUsing a single regression model allows businesses to estimate outcomes based on a single key factor, providing a simpler but less comprehensive approach compared to multiple regression."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#predicting-sales-with-a-multiple-regression-model",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#predicting-sales-with-a-multiple-regression-model",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Predicting Sales with a Multiple Regression Model",
    "text": "Predicting Sales with a Multiple Regression Model\nFitting the Model:\n\n# Fit a multiple regression model\nfit_multiple &lt;- lm(sales ~ adv_spend + prod_complex + support_rating, data = my_tech_data)\nsummary(fit_multiple)"
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#prediction-example-1",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#prediction-example-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Prediction Example",
    "text": "Prediction Example\n\n\n\n\nScenario:\nA business wants to predict sales based on the following values:\n\nAdvertising Spend: 60 (in $1,000)\nProduct Complexity: 7\nSupport Rating: 4\n\n\n# Define new data for prediction\nnew_data &lt;- data.frame(\n  adv_spend = 60, \n  prod_complex = 7, \n  support_rating = 4\n)\n\n# Predict sales\npredicted_sales &lt;- predict(fit_multiple, newdata = new_data)\npredicted_sales\n\n\n\n\n\nInput Variables: The predictor values are input into the regression equation derived from the fitted model.\nOutput: The predicted sales value is calculated based on the model’s coefficients and the provided input values.\nInterpretation: The result gives an estimate of the expected monthly revenue (in $1,000s) for the specified input conditions.\n\nKey Takeaway:\n\nUsing a multiple regression model allows businesses to integrate key factors influencing sales and predict outcomes with greater accuracy, facilitating informed decision-making."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#evaluation-metrics",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#evaluation-metrics",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Evaluation Metrics:",
    "text": "Evaluation Metrics:\n\n\n\n\nSummary Table:\n\n\n\n\n\n\n\n\nModel\nMAE\nRMSE\n\n\n\n\nSingle Regression\n6.43\n8.1\n\n\nMultiple Regression\n6.07\n7.73\n\n\n\n\n\nMean Absolute Error (MAE): Average absolute difference between predicted and actual sales.\nRoot Mean Square Error (RMSE): Measures the standard deviation of the prediction errors.\n\n\n\n\nKey Takeaways:\n\nMultiple Regression Model:\n\nTypically achieves lower MAE and RMSE, indicating better prediction accuracy.\nLeverages additional variables (e.g., product complexity and support rating) to improve performance.\n\nSingle Regression Model:\n\nProvides simpler predictions but lacks the nuance of accounting for multiple influencing factors."
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#visual-comparison-of-errors",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#visual-comparison-of-errors",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Visual Comparison of Errors",
    "text": "Visual Comparison of Errors"
  },
  {
    "objectID": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#summary-1",
    "href": "lecture_slides/07_supervised_learning_regression/07_supervised_learning_regression.html#summary-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Summary",
    "text": "Summary\n\n\nMain Takeaways from this lecture:\n\nCorrelation vs. Causation: Correlation does not imply causation.\nSimple vs. Multiple Regression: Multiple regression offers better insight by controlling for additional variables.\nModel Assumptions: Meeting assumptions (linearity, normality, etc.) is essential for reliable inference and prediction.\nSupervised vs. Unsupervised Learning: Supervised uses labeled data for prediction; unsupervised finds patterns in unlabeled data.\nBusiness Relevance: Regression models help forecast key metrics (e.g., sales) and guide strategic decisions (e.g., advertising spend, product enhancements)."
  },
  {
    "objectID": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#overview",
    "href": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#overview",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nClustering\nClustering Methods\nPreparing Data for Clustering\n\n\n\nNumber of Clusters\nK-means Clustering"
  },
  {
    "objectID": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#what-is-clustering",
    "href": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#what-is-clustering",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "What is Clustering?",
    "text": "What is Clustering?\n\nClustering is an unsupervised learning technique for finding patterns or groupings in data, without relying on predefined labels.\nCommon Business Applications:\n\nCustomer Segmentation: Identify groups of customers with similar buying behaviors or demographics.\nMarket Segmentation: Find potential submarkets for targeted marketing campaigns.\nAnomaly Detection: Detect unusual transactions or outliers in finance.\n\nKey Idea: We measure similarity (or distance) among data points and group the “closest” points together."
  },
  {
    "objectID": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#main-clustering-methods",
    "href": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#main-clustering-methods",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Main Clustering Methods",
    "text": "Main Clustering Methods\n\nChoosing a Method: The best clustering technique depends on the nature of the data, the goals of analysis, and computational constraints.\n\n\n\nMethod\nCharacteristics\nStrengths\nWeaknesses\n\n\n\n\nK-means\nDivides data into non-overlapping groups based on minimizing within-cluster variance.\nSimple and efficient for large datasets.\nSensitive to outliers and requires predefining the number of clusters (k).\n\n\nHierarchical\nBuilds a tree-like structure (dendrogram) to group data hierarchically.\nDoes not require predefining the number of clusters; visualizes relationships.\nComputationally expensive for large datasets; less scalable.\n\n\nDBSCAN\nGroups data based on density of points in a region.\nCan find arbitrarily shaped clusters and detect outliers.\nStruggles with varying densities and high-dimensional data.\n\n\nSpectral\nUses graph theory to partition data based on eigenvalues of similarity matrices.\nEffective for complex cluster structures and non-linear separations.\nComputationally intensive; requires tuning parameters like similarity metric.\n\n\nGaussian Mixture\nFits data to a probabilistic model, assuming data is generated from Gaussian distributions.\nHandles overlapping clusters and provides probabilities of cluster membership.\nSensitive to initialization and assumes Gaussian distribution."
  },
  {
    "objectID": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#generating-a-business-focused-synthetic-dataset-1",
    "href": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#generating-a-business-focused-synthetic-dataset-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Generating a Business-Focused Synthetic Dataset",
    "text": "Generating a Business-Focused Synthetic Dataset\n\n\nThis dataset simulates the type of information businesses collect to better understand and segment their customer base. Clustering these data points can reveal groups with similar characteristics, enabling targeted strategies:\n\nDemographics: Age of the customer.\nFinancials: Annual income and credit score.\nBehavioral Metrics: Online purchases, store visits, and a calculated spending score.\n\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Number of \"customers\"\nn &lt;- 198\n\n# Generate synthetic customer data with clear clustering patterns\nCustomerID       &lt;- 1:n\nAge              &lt;- c(\n  round(rnorm(n / 3, mean = 25, sd = 3)),  # Younger group\n  round(rnorm(n / 3, mean = 40, sd = 3)),  # Middle-aged group\n  round(rnorm(n / 3, mean = 55, sd = 3))   # Older group\n)\nAnnual_Income    &lt;- c(\n  round(rnorm(n / 3, mean = 30000, sd = 2000)),  # Lower-income group\n  round(rnorm(n / 3, mean = 75000, sd = 5000)),  # Middle-income group\n  round(rnorm(n / 3, mean = 120000, sd = 10000)) # Higher-income group\n)\nCredit_Score     &lt;- c(\n  round(runif(n / 3, min = 300, max = 550)),  # Poor credit score group\n  round(runif(n / 3, min = 600, max = 750)),  # Fair credit score group\n  round(runif(n / 3, min = 800, max = 850))   # Excellent credit score group\n)\nOnline_Purchases &lt;- c(\n  round(runif(n / 3, min = 15, max = 20)),      # High engagement group\n  round(runif(n / 3, min = 5, max = 15)),     # Medium engagement group\n  round(runif(n / 3, min = 0, max = 5))     # Low engagement group\n)\nStore_Visits     &lt;- c(\n  round(runif(n / 3, min = 1, max = 3)),      # Low frequency group\n  round(runif(n / 3, min = 4, max = 7)),      # Medium frequency group\n  round(runif(n / 3, min = 8, max = 10))      # High frequency group\n)\nSpending_Score   &lt;- c(\n  round(rnorm(n / 3, mean = 30, sd = 5)),     # Low spending group\n  round(rnorm(n / 3, mean = 50, sd = 5)),     # Medium spending group\n  round(rnorm(n / 3, mean = 70, sd = 5))      # High spending group\n)\n\n# Combine variables into a data frame\nbusiness_data &lt;- data.frame(\n  CustomerID,\n  Age,\n  Annual_Income,\n  Credit_Score,\n  Online_Purchases,\n  Store_Visits,\n  Spending_Score\n)\n\n# Quick exploration of the generated data\n# head(business_data)   # Preview the first few rows\n# summary(business_data) # Summary statistics for a deeper understanding"
  },
  {
    "objectID": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#scaling-data-for-clustering",
    "href": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#scaling-data-for-clustering",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Scaling Data for Clustering",
    "text": "Scaling Data for Clustering\n\n\n\nWhy Scaling Matters: Clustering methods like K-means rely on distance calculations (e.g., Euclidean distance) to group data points. Variables with larger scales (like income) can disproportionately influence the results, overshadowing variables with smaller ranges (like the number of store visits).\nScaling Process: Standardizing the data ensures that all variables contribute equally to the distance calculations. After scaling, each variable will have: Mean = 0 and Standard Deviation = 1.\nKey Outcome: Scaling ensures that no single variable dominates due to its range, enabling fair and accurate clustering. By scaling, we prepare the dataset for clustering while preserving the relationships and patterns inherent in the original data.\n\n\n# Select relevant columns for clustering (excluding the CustomerID)\nbusiness_features &lt;- business_data[, -1]\n\n# Scale data so that each feature contributes more equally\nbusiness_scaled &lt;- scale(business_features)\n\n# Check scaled data\n# head(business_scaled)  # Preview the scaled data to verify changes"
  },
  {
    "objectID": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#methods-to-determine-the-number-of-clusters",
    "href": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#methods-to-determine-the-number-of-clusters",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Methods to Determine the Number of Clusters",
    "text": "Methods to Determine the Number of Clusters\n\n\n\n\n\n\n\n\n\nMethod\nHow it Works\nWhen to Use\n\n\n\n\nElbow Method\nPlots the within-cluster sum of squares (WSS) against the number of clusters (k). The “elbow” point suggests the optimal k.\nSuitable for datasets with compact, well-separated clusters. Provides a visual heuristic but can be subjective.\n\n\nSilhouette Method\nMeasures how similar an object is to its own cluster compared to other clusters. The average silhouette score indicates clustering quality.\nIdeal when seeking well-defined clusters with clear separation. Helps assess cluster validity without relying on visual interpretation.\n\n\nGap Statistic\nCompares total within-cluster variation for different k values against random data. The k with the largest gap indicates the optimal number of clusters.\nEffective for complex datasets with no obvious structure. Computationally intensive but provides robust results.\n\n\nHierarchical Clustering (Dendrograms)\nUses a dendrogram to visualize data merging at different levels. Cut the dendrogram at the height corresponding to the desired number of clusters.\nUseful for smaller datasets or when visualizing relationships between clusters is important. Less effective for large datasets due to computational overhead.\n\n\nBayesian Information Criterion (BIC)\nEvaluates models based on statistical criteria, balancing model fit and complexity. Lower BIC scores suggest better models with appropriate cluster numbers.\nEffective for probabilistic clustering methods like Gaussian Mixture Models. Requires knowledge of model assumptions.\n\n\n\n\nHere’s the updated slide with an added explanation about how the Elbow Method works:"
  },
  {
    "objectID": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#number-of-clusters-elbow-method",
    "href": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#number-of-clusters-elbow-method",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Number of Clusters: Elbow Method",
    "text": "Number of Clusters: Elbow Method\n\n\nHow the Elbow Method Works:\n\nThe Elbow Method involves plotting the within-cluster sum of squares (WSS) against the number of clusters (k).\nWSS measures the total variance within clusters, and as the number of clusters increases, WSS decreases.\nThe “elbow point” is where the rate of decrease in WSS slows down significantly. This point suggests the optimal number of clusters (k) as it balances cluster compactness and simplicity.\n\n\n\n\n\n\n\n\n\n\nInterpretation: we might choose k = 3 based on the Elbow Method, as the WSS shows a noticeable change in slope at this point."
  },
  {
    "objectID": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#k-means-clustering-1",
    "href": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#k-means-clustering-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "K-means Clustering",
    "text": "K-means Clustering\n\n\n\nSimulation!\nhttps://www.naftaliharris.com/blog/visualizing-k-means-clustering/"
  },
  {
    "objectID": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#k-means-clustering-2",
    "href": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#k-means-clustering-2",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "K-means Clustering",
    "text": "K-means Clustering\n\n\nThe K-means algorithm begins by randomly assigning initial centroids within the data space.\n\nnstart = 25 means 25 different random initial centroid sets are tested, helping achieve a more stable solution.\n\nEach data point is then assigned to the nearest centroid, forming preliminary clusters.\nThe centroids are updated by recalculating the mean position of all points within each cluster.\nThese steps—assignment and centroid update—repeat iteratively until convergence, where cluster memberships no longer change and the centroids stabilize.\n\nkm_res$cluster returns the cluster assignment for each observation.\n\n\n\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Let's pick k=3 based on the elbow plot\nkm_res &lt;- kmeans(business_scaled, centers = 3, nstart = 25)\n\n# Examine the results\n# km_res\n# head(km_res$cluster)"
  },
  {
    "objectID": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#visualizing-k-means-clusters",
    "href": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#visualizing-k-means-clusters",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Visualizing K-means Clusters",
    "text": "Visualizing K-means Clusters\n\n\n\n\n\n\n\n\n\n\n\nThe plot maps high-dimensional data into 2D using PCA (Principal Component Analysis) to make it easier to interpret.\nEach cluster formed in the original high-dimensional space is projected into 2D and represented by a unique color and surrounded by an ellipse showing the confidence region.\nCentroids (cluster centers) are marked to summarize each group."
  },
  {
    "objectID": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#interpreting-clusters-for-business",
    "href": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#interpreting-clusters-for-business",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Interpreting Clusters for Business",
    "text": "Interpreting Clusters for Business\n\n\n# Add cluster info to the original data\nbusiness_data$Cluster &lt;- km_res$cluster\n\n# Calculate average values per cluster\ncluster_eda &lt;- aggregate(. ~ Cluster, data = business_data[, -1], FUN = mean)\n# cluster_eda\n\nInsights:\n\n\n\n\n\n\n\n\n\nFeature\nCluster 1\nCluster 2\nCluster 3\n\n\n\n\nAge\nOlder (~54.86)\nMiddle-aged (~40.05)\nYoungest (~25.08)\n\n\nAnnual Income\nHighest (~$119,371)\nModerate (~$76,047.36)\nLowest (~$29,954.50)\n\n\nCredit Score\nExcellent (~824.67)\nModerate (~676.21)\nLowest (~418.36)\n\n\nOnline Purchases\nLowest (~2.45)\nModerate (~9.94)\nHighest (~17.38)\n\n\nStore Visits\nHighest (~8.88 visits)\nModerate (~5.51 visits)\nLowest (~1.97 visits)\n\n\nSpending Score\nHigh (~70.56)\nModerate (~49.97)\nLow (~30.76)\n\n\n\nBusiness Potential Implications:\n\nCluster 1: Target high-income customers with store-centric promotions.\nCluster 2: Leverage moderate online and in-store engagement for balanced campaigns.\nCluster 3: Focus on e-commerce strategies to engage younger, lower-income customers."
  },
  {
    "objectID": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#summary-1",
    "href": "lecture_slides/08_unsupervised_learning_cluster/08_unsupervised_learning_cluster.html#summary-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Summary",
    "text": "Summary\n\n\nMain Takeaways from this lecture:\n\n\n\nClustering is an unsupervised learning method that groups data based on similarity, without pre-existing labels.\nK-means is a widely used technique that requires:\n\nChoosing the number of clusters (k), often guided by the Elbow Method or other metrics.\nScaling data so no variable dominates due to differing scales.\n\nA business-focused synthetic dataset (Age, Income, Credit Score, etc.) demonstrates how clustering reveals customer segments with distinct demographic, financial, and behavioral traits.\nCluster interpretation involves examining average features per cluster and translating these differences into actionable insights (e.g., targeted marketing, store vs. online promotions).\nOther clustering methods (Hierarchical, DBSCAN, Spectral, Gaussian Mixtures) may offer advantages for different data structures or domain needs."
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#overview",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#overview",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nIntroduction to Logistic Regression\nTrain-Test Split\nModel Training: Fitting a Logistic Regression Model\n\nPredictions on the Test Set\n\n\n\nModel Evaluation\n\nConfusion Matrix\nSensitivity\nSpecificity\nPrecision\nF1 Score\nBalanced Accuracy\nROC Curve & AUC\n\nK-Fold Cross-Validation"
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#what-is-a-classification-problem",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#what-is-a-classification-problem",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "What is a classification problem?",
    "text": "What is a classification problem?\n\n\n\n\nClassification involves categorizing data into predefined classes or groups based on their features."
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#what-is-logistic-regression",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#what-is-logistic-regression",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "What is Logistic Regression?",
    "text": "What is Logistic Regression?\n\nA statistical method used for binary classification tasks.\nPredicts the probability of an event occurring (e.g., Yes or No).\nUseful for tasks like determining if a customer will make a purchase."
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#why-logistic-regression",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#why-logistic-regression",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Why Logistic Regression?",
    "text": "Why Logistic Regression?\n\nHandles binary outcomes effectively.\nProvides interpretable results (log-odds, odds ratios, and probability).\nCan handle continuous and categorical predictors."
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#assumptions",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#assumptions",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Assumptions",
    "text": "Assumptions\n\nBinary Dependent Variable: Logistic regression predicts probabilities for two categories.\nIndependent Observations: Data points should be independent of each other.\nLinearity in the Log-Odds: The log-odds of the dependent variable should be linearly related to the independent variables.\nNo Multicollinearity: Independent variables should not be highly correlated."
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#the-logistic-regression-model",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#the-logistic-regression-model",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "The Logistic Regression Model",
    "text": "The Logistic Regression Model\nLog-Odds Equation\n\\[\n\\text{logit}(P) = \\log\\left(\\frac{P}{1-P}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_nX_n\n\\]\n\n\n\n\\(P\\): Probability of the outcome (e.g., Purchase = Yes).\n\\(X_1, X_2, \\dots, X_n\\): Predictor variables.\n\\(\\beta_0, \\beta_1, \\dots, \\beta_n\\): Coefficients.\n\n\n\nConverting Log-Odds to Probability\n\\[\nP = \\frac{1}{1 + e^{-\\text{logit}(P)}}\n\\]"
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#generating-a-synthetic-business-dataset-1",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#generating-a-synthetic-business-dataset-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Generating a Synthetic Business Dataset",
    "text": "Generating a Synthetic Business Dataset\n\n\nIn many business scenarios, we aim to predict whether a customer will make a purchase based on collected information, usually on demographic and engagement factors. Our synthetic dataset simulates a simplified version of real-world data, capturing:\n\nAge: Customer’s age, distributed around 40 years old with some variation\n\nGender: Categorical variable indicating \"Male\" or \"Female\"\n\nIncome: Approximate annual income in US dollars\n\nEngagement: A normalized score (0 to 1) reflecting how actively a customer interacts with the business (e.g., website visits, email clicks, etc.)\n\nPurchase: A binary outcome (\"No\" or \"Yes\") indicating whether the customer decided to buy a product"
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#generating-a-synthetic-business-dataset-2",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#generating-a-synthetic-business-dataset-2",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Generating a Synthetic Business Dataset",
    "text": "Generating a Synthetic Business Dataset\n\n# Uncomment and install if needed:\n# install.packages(\"caret\")\n# install.packages(\"dplyr\")\n# install.packages(\"pROC\")\n\nlibrary(caret)\nlibrary(dplyr)\nlibrary(pROC)\n\nset.seed(123)  # ensures reproducibility\n\nn &lt;- 1000\n\n# 1) Generate basic customer attributes\nAge &lt;- round(rnorm(n, mean = 40, sd = 10))\nGender &lt;- sample(c(\"Male\", \"Female\"), n, replace = TRUE)\nIncome &lt;- round(rnorm(n, mean = 60000, sd = 10000))\nEngagement &lt;- runif(n, min = 0, max = 1)\n\n# 2) Make some feature shifts to mimic demographic differences\n#    (Optional step to create more distinct sub-groups)\nEngagement[Gender == \"Female\"] &lt;- Engagement[Gender == \"Female\"] + 0.1\nIncome[Gender == \"Female\"]     &lt;- Income[Gender == \"Female\"] + 3000\nAge[Gender == \"Female\"]        &lt;- Age[Gender == \"Female\"] + 1\n\n# 3) Define business rules / assumptions for purchase probability\n#    We'll combine a baseline with increments/decrements based on thresholds.\nbaseline_prob &lt;- 0.2\n\npurchase_prob &lt;- rep(baseline_prob, n)\n\n# a) Customers with higher engagement more likely to buy\npurchase_prob[Engagement &gt;= 0.5] &lt;- purchase_prob[Engagement &gt;= 0.5] + 0.3\n\n# b) Higher income is a strong indicator of potential purchase\npurchase_prob[Income &gt;= 55000] &lt;- purchase_prob[Income &gt;= 55000] + 0.25\n\n# c) Suppose 'Female' customers have a slight bump in probability\npurchase_prob[Gender == \"Female\"] &lt;- purchase_prob[Gender == \"Female\"] + 0.1\n\n# d) Age effect: perhaps older customers (&gt;=45) have slightly higher probability\npurchase_prob[Age &gt;= 45] &lt;- purchase_prob[Age &gt;= 45] + 0.05\n\n# 4) Add random noise to reflect real-world unpredictability\n#    (some random variation that might increase or decrease probabilities)\nnoise &lt;- rnorm(n, mean = 0, sd = 0.05)\npurchase_prob &lt;- purchase_prob + noise\n\n# 5) Clamp probabilities to valid [0, 1] range\npurchase_prob &lt;- pmin(pmax(purchase_prob, 0.01), 0.99)\n\n# 6) Generate the Purchase variable via Bernoulli trials\nPurchase &lt;- rbinom(n, size = 1, prob = purchase_prob)\n\n# 7) Combine into a data frame\ndf &lt;- data.frame(\n  Age        = Age,\n  Gender     = factor(Gender),\n  Income     = Income,\n  Engagement = Engagement,\n  Purchase   = factor(Purchase, labels = c(\"No\", \"Yes\"))\n)\n\n# Quick checks\ntable(df$Purchase)\n\nhead(df)"
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#summary-statistics",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#summary-statistics",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\nsummary(df)"
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#age-vs.-purchase-probability",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#age-vs.-purchase-probability",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Age vs. Purchase Probability",
    "text": "Age vs. Purchase Probability"
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#income-vs.-purchase-probability",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#income-vs.-purchase-probability",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Income vs. Purchase Probability",
    "text": "Income vs. Purchase Probability"
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#purchase-rate-by-gender",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#purchase-rate-by-gender",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Purchase Rate by Gender",
    "text": "Purchase Rate by Gender"
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#purchase-probability-by-engagement",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#purchase-probability-by-engagement",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Purchase Probability by Engagement",
    "text": "Purchase Probability by Engagement"
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#correlation-matrix",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#correlation-matrix",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Correlation Matrix",
    "text": "Correlation Matrix"
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#train-test-split-1",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#train-test-split-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Train-Test Split",
    "text": "Train-Test Split\n\nRandomly split the data into train (80%) and test (20%) for unbiased evaluation. This process ensures that the model is trained on one subset of the data and evaluated on another subset, mimicking real-world scenarios where predictions are made on unseen data. It helps to assess how well the model generalizes to new information.\n\nset.seed(123)\n\ntrain_index &lt;- createDataPartition(df$Purchase, p = 0.8, list = FALSE)\ntrain_data &lt;- df[train_index, ]\ntest_data  &lt;- df[-train_index, ]\n\ntable(train_data$Purchase)"
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#baseline-prediction",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#baseline-prediction",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Baseline Prediction",
    "text": "Baseline Prediction\n\n\nThe baseline prediction is the proportion of customers who made a purchase:\n\nbaseline_prob &lt;- mean(train_data$Purchase == \"Yes\")\nbaseline_prob\n\nInterpretation:\n\nThis baseline represents the probability of a purchase if no predictors are used.\nIt serves as a reference point to evaluate the added value of the logistic regression model."
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#fitting-a-logistic-regression-model",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#fitting-a-logistic-regression-model",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Fitting a Logistic Regression Model",
    "text": "Fitting a Logistic Regression Model\n\n\nWe fit a logistic regression model on the training data and model the relationship between a binary dependent variable and one or more independent variables. Here, we predict the probability of purchase (Purchase) based on customer attributes (Age, Gender, Income, and Engagement).\nThis step includes specifying:\n\nThe formula (Purchase ~ Age + Gender + Income + Engagement), which defines the relationship between the dependent and independent variables.\nThe method (glm), indicating that we are using a generalized linear model.\nThe link function (binomial(link = \"logit\")), ensuring the model estimates probabilities using the logistic function.\n\n\nset.seed(123)\n\ntrain_control &lt;- trainControl(\n  classProbs      = TRUE,\n  summaryFunction = prSummary    # &lt;--- uses Precision, Recall, F, AUC\n)\n\nmodel_fit &lt;- train(\n  Purchase ~ Age + Gender + Income + Engagement,\n  data = train_data,\n  method = \"glm\",\n  family = binomial(link = \"logit\"),\n  trControl = train_control,\n  metric    = \"F\"          # or \"AUC\", \"Precision\", \"Recall\"\n)\n\nsummary(model_fit$finalModel)\n\nThe summary provides details about the model’s coefficients, which indicate the strength and direction of each predictor’s influence on the purchase probability."
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#predictions-on-the-test-set-1",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#predictions-on-the-test-set-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Predictions on the Test Set",
    "text": "Predictions on the Test Set\n\n\nTo evaluate the model, we obtain predictions for the test set. This involves two steps:\n\nClass Predictions: Predicting the most likely class (e.g., Yes or No) for each observation.\nProbability Predictions: Predicting the probability of each class, which provides more nuanced insights.\n\nThis allows us to assess the model’s ability to generalize to unseen data and evaluate its performance using metrics such as accuracy, ROC-AUC, and confusion matrix.\n\npred_classes &lt;- predict(model_fit, newdata = test_data)\npred_probs   &lt;- predict(model_fit, newdata = test_data, type = \"prob\")\n\nhead(pred_classes)  # Displays the predicted classes for the first few observations\nhead(pred_probs)    # Displays the predicted probabilities for the first few observations\n\nBy analyzing these predictions, we can determine how well the model distinguishes between Yes and No cases and identify areas for improvement."
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#confusion-matrix",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#confusion-matrix",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\n\nThe confusion matrix provides a summary of prediction results. It compares the predicted and actual classes, offering insights into the model’s classification performance.\n\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)"
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#confusion-matrix-1",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#confusion-matrix-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\n\nKey metrics derived from the confusion matrix include:\n\n\n\nPerformance Metrics\n\n\nMetric\nValue\n\n\n\n\nAccuracy\n0.6950\n\n\nSensitivity (Recall)\n0.8492\n\n\nSpecificity\n0.4324\n\n\nPrecision\n0.7181\n\n\nF1 Score\n0.7782\n\n\n\n\n\n\n\nAccuracy: The proportion of correct predictions.\nSensitivity (Recall): The model’s ability to identify positive cases.\nSpecificity: The model’s ability to identify negative cases.\nPrecision: Among predicted positive cases, the proportion that are truly positive.\nF1 Score: The harmonic mean of Precision and Sensitivity, balancing false positives and false negatives."
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#accuracy",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#accuracy",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Accuracy",
    "text": "Accuracy\n\n\n\n\n\nConfusion Matrix\n\n\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)\n\n\n\n\n\nFormula\n\n\n\\[\n\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n\\]\n\n\n\n\nOverall effectiveness of the model.\nIn the context of weather forecasting, for example, accuracy reflects how well a model predicts weather events correctly (e.g., rainy or sunny days).\nHigh accuracy: lots of correct predictions!"
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#recall-sensitivity",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#recall-sensitivity",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Recall (Sensitivity)",
    "text": "Recall (Sensitivity)\n\n\n\n\n\nConfusion Matrix\n\n\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)\n\n\n\n\n\nFormula\n\n\n\\[\n\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\\]\n\n\n\n\nIs the fraction of positives correctly identified.\nIn criminal justice, it would assess how well a predictive policing model identifies all potential criminal activities (True Positives) without missing any (thus minimizing False Negatives).\nHigh recall: low false-negative rates."
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#specificity",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#specificity",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Specificity",
    "text": "Specificity\n\n\n\n\n\nConfusion Matrix\n\n\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)\n\n\n\n\n\nFormula\n\n\n\\[\n\\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}\n\\]\n\n\n\n\nIt is the true negative rate, measuring a model’s ability to correctly identify actual negatives.\nCrucial in fields where incorrectly identifying a negative case as positive could have serious implications (e.g., criminal justice).\nHigh specificity: the model is very effective at identifying true negatives."
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#precision",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#precision",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Precision",
    "text": "Precision\n\n\n\n\n\nConfusion Matrix\n\n\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)\n\n\n\n\n\nFormula\n\n\n\\[\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n\\]\n\n\n\n\nAccuracy of positive predictions.\nIn email spam detection, it would indicate the percentage of emails correctly identified as spam (True Positives) out of all emails flagged as spam, aiming to reduce the number of legitimate emails incorrectly marked as spam (False Positives).\nHigh precision: low false-positive rates."
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#f1-score",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#f1-score",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "F1-Score",
    "text": "F1-Score\n\n\n\n\n\nConfusion Matrix\n\n\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)\n\n\n\n\n\nFormula\n\n\n\\[\n\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\n\n\n\n\nHarmonic mean of Precision and Recall.\nIn a medical diagnosis scenario, it would help in evaluating a test’s effectiveness in correctly identifying patients with a disease (True Positives) while minimizing the misclassification of healthy individuals as diseased (False Positives and False Negatives).\nHigh F1 score: a better balance between precision and recall."
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#roc-curve-auc-1",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#roc-curve-auc-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "ROC Curve & AUC",
    "text": "ROC Curve & AUC\n\n\nThe ROC (Receiver Operating Characteristic) curve is a graphical representation that illustrates the trade-off between the true positive rate (Sensitivity) and the false positive rate (1 - Specificity) across various threshold values. The AUC (Area Under the Curve) summarizes the ROC curve into a single value, indicating the model’s overall performance.\n\nAUC Interpretation:\n\nAUC \\(\\approx\\) 1: The model has near-perfect predictive capability.\nAUC \\(\\approx\\) 0.5: The model performs no better than random guessing."
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#roc-curve-auc-2",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#roc-curve-auc-2",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "ROC Curve & AUC",
    "text": "ROC Curve & AUC\n\n\nThe ROC curve and AUC provide a comprehensive evaluation of the model’s discriminatory power.\n\n\n\n\n\n\n\n\n\nAUC Value: Indicates the likelihood that the model will correctly distinguish between a positive and a negative instance. Higher values reflect better model performance."
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#k-fold-cross-validation-1",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#k-fold-cross-validation-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "K-Fold Cross-Validation",
    "text": "K-Fold Cross-Validation\n\n\nK-Fold Cross-Validation is a resampling technique that evaluates model performance by dividing the dataset into k subsets (folds). The model is trained on k-1 folds and validated on the remaining fold, repeating this process \\(k\\) times to ensure each fold serves as the validation set once.\nAdvantages of K-Fold Cross-Validation:\n\nReduces overfitting by training and validating on different subsets.\nProvides a more reliable estimate of model performance compared to a single train-test split.\nEnsures the model generalizes well to unseen data."
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#k-fold-cross-validation-2",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#k-fold-cross-validation-2",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "K-Fold Cross-Validation",
    "text": "K-Fold Cross-Validation\n\n# install.packages(\"MLmetrics\")\nlibrary(MLmetrics)\n\nset.seed(123)\n\ntrain_control &lt;- trainControl(\n  method          = \"cv\",\n  number          = 10,\n  p = .8,\n  classProbs      = TRUE,\n  summaryFunction = prSummary    # &lt;--- uses Precision, Recall, F, AUC\n)\n\nmodel_cv &lt;- train(\n  Purchase ~ .,\n  data      = train_data,  # or your entire df, depending on your design\n  method    = \"glm\",\n  family = binomial(link = \"logit\"),\n  trControl = train_control,\n  metric    = \"F\"          # or \"AUC\", \"Precision\", \"Recall\"\n)\n\n# Inspect the cross-validation results:\n# model_cv  # Displays cross-validated model performance\nmodel_cv$results"
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#comparing-train-test-performance-and-k-fold-cross-validation-performance-1",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#comparing-train-test-performance-and-k-fold-cross-validation-performance-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Comparing Train-Test Performance and K-Fold Cross-Validation Performance",
    "text": "Comparing Train-Test Performance and K-Fold Cross-Validation Performance\n\n\n\n\n\n\nAssessing Model Generalization\n\nTrain-Test Performance evaluates how well a model generalizes to unseen data from a single split.\nK-Fold Cross-Validation provides a more robust performance estimate across multiple splits.\nComparing the two ensures consistent performance on unseen data.\n\n\n\n\n\nDetecting Overfitting or Underfitting\n\nOverfitting: Train-test performance is much better than K-Fold performance.\nUnderfitting: Both metrics show poor performance.\nThe comparison highlights issues with model complexity.\n\n\n\n\n\nMitigating Sampling Bias\n\nSingle splits (train-test) can introduce sampling bias, especially with small datasets.\nCross-validation averages performance across folds, reducing bias.\nEnsures reliability of performance estimates.\n\n\n\n\n\n\nUnderstanding Variance in Model Evaluation\n\nTrain-test performance can vary based on split choice.\nK-Fold Cross-Validation offers insights into the variance of the performance estimate.\nAids in strategies to reduce variance, such as adding more data.\n\n\n\n\n\nModel Selection and Hyperparameter Tuning\n\nCross-validation is used for stable performance estimates during training.\nTrain-Test Performance validates the chosen model’s final performance.\nDiscrepancies indicate potential overfitting during tuning.\n\n\n\n\n\nReal-World Applicability\n\nTrain-Test: Simpler, faster, checks practical performance on unseen data.\nK-Fold Cross-Validation: Provides theoretical robustness.\nBridging evaluation theory and deployment performance."
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#comparing-train-test-performance-and-k-fold-cross-validation-performance-2",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#comparing-train-test-performance-and-k-fold-cross-validation-performance-2",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Comparing Train-Test Performance and K-Fold Cross-Validation Performance",
    "text": "Comparing Train-Test Performance and K-Fold Cross-Validation Performance\n\n# =========================\n# Load required packages\n# =========================\nlibrary(caret)\nlibrary(pROC)\n\n# =========================\n# Single Train-Test Performance\n# =========================\n\n# 1. Compute confusion matrix to get Precision, Recall, F1\ntrain_test_metrics &lt;- confusionMatrix(\n  data      = pred_classes,\n  reference = test_data$Purchase,\n  positive  = \"Yes\"\n)\n\n# 2. Calculate AUC on the test set\nroc_obj_test &lt;- roc(test_data$Purchase, pred_probs$Yes, levels = c(\"No\", \"Yes\"))\nauc_test     &lt;- as.numeric(auc(roc_obj_test))\n\n# 3. Obtain 95% CI for the AUC\nauc_test_ci  &lt;- ci(roc_obj_test, of = \"auc\")  # typically returns c(lower, median, upper)\n\n# 4. Extract the relevant metrics\ntrain_test_results &lt;- list(\n  AUC        = auc_test,\n  Precision  = train_test_metrics$byClass[[\"Pos Pred Value\"]],\n  Recall     = train_test_metrics$byClass[[\"Sensitivity\"]],\n  F1         = train_test_metrics$byClass[[\"F1\"]]\n)\n\n# =========================\n# K-Fold Cross-Validation Metrics\n# =========================\n\n# Identify the row with the highest F in model_cv\nbest_idx  &lt;- which.max(model_cv$results$F)\ncv_metrics &lt;- model_cv$results[best_idx, ]\n\n# Assuming model_cv$results contains columns like:\n#  AUC, AUCSD, Precision, PrecisionSD, Recall, RecallSD, F, FSD\n# (from a custom prSummary or similar)\ncv_results &lt;- list(\n  AUC             = cv_metrics[[\"AUC\"]],\n  AUC_lower       = cv_metrics[[\"AUC\"]] - 1.96 * cv_metrics[[\"AUCSD\"]],\n  AUC_upper       = cv_metrics[[\"AUC\"]] + 1.96 * cv_metrics[[\"AUCSD\"]],\n  Precision       = cv_metrics[[\"Precision\"]],\n  Precision_lower = cv_metrics[[\"Precision\"]] - 1.96 * cv_metrics[[\"PrecisionSD\"]],\n  Precision_upper = cv_metrics[[\"Precision\"]] + 1.96 * cv_metrics[[\"PrecisionSD\"]],\n  Recall          = cv_metrics[[\"Recall\"]],\n  Recall_lower    = cv_metrics[[\"Recall\"]] - 1.96 * cv_metrics[[\"RecallSD\"]],\n  Recall_upper    = cv_metrics[[\"Recall\"]] + 1.96 * cv_metrics[[\"RecallSD\"]],\n  F1              = cv_metrics[[\"F\"]],\n  F1_lower        = cv_metrics[[\"F\"]] - 1.96 * cv_metrics[[\"FSD\"]],\n  F1_upper        = cv_metrics[[\"F\"]] + 1.96 * cv_metrics[[\"FSD\"]]\n)\n\n# =========================\n# Combine Results\n# =========================\ncomparison &lt;- list(\n  Train_Test = train_test_results,\n  K_Fold_CV  = cv_results\n)\n\ncomparison"
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#visualizing-single-train-test-vs-k-fold-cv",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#visualizing-single-train-test-vs-k-fold-cv",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Visualizing Single Train-Test vs K-Fold CV",
    "text": "Visualizing Single Train-Test vs K-Fold CV"
  },
  {
    "objectID": "lecture_slides/09_model_evaluation/09_model_evaluation.html#summary-1",
    "href": "lecture_slides/09_model_evaluation/09_model_evaluation.html#summary-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Summary",
    "text": "Summary\n\n\nMain Takeaways from this lecture:\n\n\nLogistic Regression Framework - Logistic regression is a powerful method for binary classification. - Key assumptions: - Binary dependent variable. - Linearity in log-odds. - No multicollinearity among predictors.\nEvaluation Metrics\n\nConfusion Matrix provides:\n\nSensitivity (Recall)\nSpecificity\nPrecision\nF1 Score\n\nROC Curve and AUC evaluate model discrimination across thresholds.\n\n\n\nTrain-Test Split:\n\nEnsures the model is trained and tested on separate datasets.\nBaseline accuracy serves as a benchmark for improvement.\n\nK-Fold Cross-Validation:\n\nReduces overfitting and sampling bias.\nProvides a reliable estimate of model performance.\n\n\nPerformance Comparison\n\nCross-validation metrics give a robust view of generalization.\nTrain-test split results provide practical insights for deployment.\nUse both approaches for a comprehensive evaluation."
  }
]